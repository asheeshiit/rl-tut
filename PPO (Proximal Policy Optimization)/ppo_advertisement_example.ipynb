{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ PPO (Proximal Policy Optimization) for Advertisement Optimization\n",
        "\n",
        "This notebook demonstrates PPO algorithm using an **Advertisement/Ads Domain** example.\n",
        "\n",
        "## Business Problem\n",
        "We have an ad platform that needs to decide **which ad to show** to maximize **click-through rate (CTR)**.\n",
        "\n",
        "### RL Framework Mapping:\n",
        "| RL Concept | Ads Domain Equivalent |\n",
        "|------------|----------------------|\n",
        "| **Agent** | Ad Recommendation System |\n",
        "| **State** | User features (age, interests, device, time) |\n",
        "| **Action** | Which ad to display (Ad A, B, C, D) |\n",
        "| **Reward** | +1 for click, 0 for no click |\n",
        "| **Policy** | Strategy to select ads based on user features |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üì¶ Step 1: Import Required Libraries\n",
        "\n",
        "We import essential libraries for:\n",
        "- **NumPy**: Numerical computations\n",
        "- **PyTorch**: Building neural networks for policy and value functions\n",
        "- **Matplotlib**: Visualization of training progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Step 2: Create Dummy Advertisement Data\n",
        "\n",
        "We create a simulated advertisement environment with:\n",
        "- **User Features**: Age group, interest category, device type, time of day\n",
        "- **Available Ads**: 4 different ads (Sports, Tech, Fashion, Food)\n",
        "- **Click Probabilities**: Each user segment has different preferences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã State Dimension: 15\n",
            "üé¨ Action Dimension (Number of Ads): 4\n",
            "\n",
            "üë§ User Features:\n",
            "   Age Groups: ['18-25', '26-35', '36-50', '50+']\n",
            "   Interests: ['Sports', 'Tech', 'Fashion', 'Food']\n",
            "   Devices: ['Mobile', 'Desktop', 'Tablet']\n",
            "   Time Slots: ['Morning', 'Afternoon', 'Evening', 'Night']\n",
            "\n",
            "üì¢ Available Ads: ['Sports_Ad', 'Tech_Ad', 'Fashion_Ad', 'Food_Ad']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DUMMY DATA CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# User feature dimensions\n",
        "# State = [age_group, interest, device, time_of_day]\n",
        "# Each feature is one-hot encoded\n",
        "\n",
        "AGE_GROUPS = ['18-25', '26-35', '36-50', '50+']      # 4 categories\n",
        "INTERESTS = ['Sports', 'Tech', 'Fashion', 'Food']    # 4 categories  \n",
        "DEVICES = ['Mobile', 'Desktop', 'Tablet']            # 3 categories\n",
        "TIME_SLOTS = ['Morning', 'Afternoon', 'Evening', 'Night']  # 4 categories\n",
        "\n",
        "# Available ads to show\n",
        "ADS = ['Sports_Ad', 'Tech_Ad', 'Fashion_Ad', 'Food_Ad']  # 4 actions\n",
        "\n",
        "# State dimension = 4 + 4 + 3 + 4 = 15 (one-hot encoded)\n",
        "STATE_DIM = len(AGE_GROUPS) + len(INTERESTS) + len(DEVICES) + len(TIME_SLOTS)\n",
        "ACTION_DIM = len(ADS)\n",
        "\n",
        "print(f\"üìã State Dimension: {STATE_DIM}\")\n",
        "print(f\"üé¨ Action Dimension (Number of Ads): {ACTION_DIM}\")\n",
        "print(f\"\\nüë§ User Features:\")\n",
        "print(f\"   Age Groups: {AGE_GROUPS}\")\n",
        "print(f\"   Interests: {INTERESTS}\")\n",
        "print(f\"   Devices: {DEVICES}\")\n",
        "print(f\"   Time Slots: {TIME_SLOTS}\")\n",
        "print(f\"\\nüì¢ Available Ads: {ADS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üåç Step 3: Create the Advertisement Environment\n",
        "\n",
        "The environment simulates user behavior:\n",
        "1. **Generates random users** with different features\n",
        "2. **Computes click probability** based on user-ad match\n",
        "3. **Returns reward** (+1 click, 0 no-click)\n",
        "\n",
        "### Click Probability Logic:\n",
        "- Users are more likely to click ads matching their interests\n",
        "- Young users (18-25) prefer Tech and Fashion\n",
        "- Mobile users have slightly lower engagement\n",
        "- Evening time has higher engagement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Environment:\n",
            "   Initial State Shape: (15,)\n",
            "   User Info: Age=36-50, Interest=Fashion, Device=Tablet, Time=Afternoon\n",
            "\n",
            "   Action: Show Food_Ad\n",
            "   Reward: 0.0 (Clicked: False)\n",
            "   Click Probability was: 0.20\n"
          ]
        }
      ],
      "source": [
        "class AdvertisementEnvironment:\n",
        "    \"\"\"\n",
        "    Simulated Advertisement Environment\n",
        "    \n",
        "    This environment simulates user interactions with ads.\n",
        "    The agent (ad system) observes user features and decides which ad to show.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # ============================================================\n",
        "        # CLICK PROBABILITY MATRIX\n",
        "        # Rows: User interest, Columns: Ad type\n",
        "        # Higher values = more likely to click\n",
        "        # ============================================================\n",
        "        self.base_click_prob = np.array([\n",
        "            # Sports_Ad  Tech_Ad  Fashion_Ad  Food_Ad\n",
        "            [0.7,        0.2,     0.1,        0.3],   # Sports interest\n",
        "            [0.1,        0.8,     0.2,        0.2],   # Tech interest\n",
        "            [0.1,        0.3,     0.75,       0.2],   # Fashion interest\n",
        "            [0.2,        0.1,     0.15,       0.8],   # Food interest\n",
        "        ])\n",
        "        \n",
        "        self.current_state = None\n",
        "        self.current_user_info = None\n",
        "        \n",
        "    def _one_hot_encode(self, age_idx, interest_idx, device_idx, time_idx):\n",
        "        \"\"\"\n",
        "        Create one-hot encoded state vector from user features\n",
        "        \"\"\"\n",
        "        state = np.zeros(STATE_DIM)\n",
        "        \n",
        "        # One-hot encode each feature\n",
        "        offset = 0\n",
        "        state[offset + age_idx] = 1.0\n",
        "        offset += len(AGE_GROUPS)\n",
        "        \n",
        "        state[offset + interest_idx] = 1.0\n",
        "        offset += len(INTERESTS)\n",
        "        \n",
        "        state[offset + device_idx] = 1.0\n",
        "        offset += len(DEVICES)\n",
        "        \n",
        "        state[offset + time_idx] = 1.0\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Generate a new random user (new episode)\n",
        "        Returns: state (user features as one-hot vector)\n",
        "        \"\"\"\n",
        "        # Randomly sample user features\n",
        "        age_idx = np.random.randint(0, len(AGE_GROUPS))\n",
        "        interest_idx = np.random.randint(0, len(INTERESTS))\n",
        "        device_idx = np.random.randint(0, len(DEVICES))\n",
        "        time_idx = np.random.randint(0, len(TIME_SLOTS))\n",
        "        \n",
        "        # Store user info for reward calculation\n",
        "        self.current_user_info = {\n",
        "            'age': age_idx,\n",
        "            'interest': interest_idx,\n",
        "            'device': device_idx,\n",
        "            'time': time_idx\n",
        "        }\n",
        "        \n",
        "        # Create state\n",
        "        self.current_state = self._one_hot_encode(age_idx, interest_idx, device_idx, time_idx)\n",
        "        \n",
        "        return self.current_state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute action (show ad) and get reward (click/no-click)\n",
        "        \n",
        "        Args:\n",
        "            action: Index of ad to show (0-3)\n",
        "            \n",
        "        Returns:\n",
        "            next_state: New user features (new user arrives)\n",
        "            reward: 1 if user clicked, 0 otherwise\n",
        "            done: True (each user interaction is one episode)\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # ============================================================\n",
        "        # CALCULATE CLICK PROBABILITY\n",
        "        # ============================================================\n",
        "        interest_idx = self.current_user_info['interest']\n",
        "        base_prob = self.base_click_prob[interest_idx, action]\n",
        "        \n",
        "        # Modifiers based on other features\n",
        "        # Young users (18-25) get +10% for Tech and Fashion\n",
        "        if self.current_user_info['age'] == 0 and action in [1, 2]:\n",
        "            base_prob = min(1.0, base_prob + 0.1)\n",
        "        \n",
        "        # Mobile users have slightly lower engagement (-5%)\n",
        "        if self.current_user_info['device'] == 0:\n",
        "            base_prob = max(0.0, base_prob - 0.05)\n",
        "        \n",
        "        # Evening time has higher engagement (+10%)\n",
        "        if self.current_user_info['time'] == 2:\n",
        "            base_prob = min(1.0, base_prob + 0.1)\n",
        "        \n",
        "        # ============================================================\n",
        "        # SIMULATE CLICK (Bernoulli trial)\n",
        "        # ============================================================\n",
        "        clicked = np.random.random() < base_prob\n",
        "        reward = 1.0 if clicked else 0.0\n",
        "        \n",
        "        # Episode ends after one interaction (new user arrives)\n",
        "        done = True\n",
        "        next_state = self.reset()  # New user arrives\n",
        "        \n",
        "        info = {\n",
        "            'clicked': clicked,\n",
        "            'click_prob': base_prob,\n",
        "            'ad_shown': ADS[action]\n",
        "        }\n",
        "        \n",
        "        return next_state, reward, done, info\n",
        "\n",
        "# Create environment instance\n",
        "env = AdvertisementEnvironment()\n",
        "\n",
        "# Test the environment\n",
        "print(\"üß™ Testing Environment:\")\n",
        "state = env.reset()\n",
        "print(f\"   Initial State Shape: {state.shape}\")\n",
        "print(f\"   User Info: Age={AGE_GROUPS[env.current_user_info['age']]}, \"\n",
        "      f\"Interest={INTERESTS[env.current_user_info['interest']]}, \"\n",
        "      f\"Device={DEVICES[env.current_user_info['device']]}, \"\n",
        "      f\"Time={TIME_SLOTS[env.current_user_info['time']]}\")\n",
        "\n",
        "# Take a random action\n",
        "action = np.random.randint(0, ACTION_DIM)\n",
        "next_state, reward, done, info = env.step(action)\n",
        "print(f\"\\n   Action: Show {ADS[action]}\")\n",
        "print(f\"   Reward: {reward} (Clicked: {info['clicked']})\")\n",
        "print(f\"   Click Probability was: {info['click_prob']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üß† Step 4: Define the Actor-Critic Neural Network\n",
        "\n",
        "PPO uses an **Actor-Critic** architecture:\n",
        "\n",
        "| Component | Role | Output |\n",
        "|-----------|------|--------|\n",
        "| **Actor (Policy Network)** | Decides which ad to show | Probability distribution over ads |\n",
        "| **Critic (Value Network)** | Estimates how good current state is | Single value (expected reward) |\n",
        "\n",
        "### Why Both?\n",
        "- **Actor** learns the optimal ad selection strategy\n",
        "- **Critic** helps reduce variance in learning (tells actor how good its choices were)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Actor-Critic Test:\n",
            "   Selected Ad: Sports_Ad\n",
            "   State Value: 0.0470\n"
          ]
        }
      ],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    Actor-Critic Network for PPO\n",
        "    \n",
        "    Architecture:\n",
        "    - Shared base layers (feature extraction)\n",
        "    - Actor head (outputs action probabilities)\n",
        "    - Critic head (outputs state value)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        # ============================================================\n",
        "        # SHARED LAYERS - Extract features from user state\n",
        "        # ============================================================\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # ============================================================\n",
        "        # ACTOR HEAD (Policy Network) - Outputs probability of each ad\n",
        "        # ============================================================\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        \n",
        "        # ============================================================\n",
        "        # CRITIC HEAD (Value Network) - Outputs estimated value\n",
        "        # ============================================================\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        shared_features = self.shared(state)\n",
        "        action_probs = self.actor(shared_features)\n",
        "        state_value = self.critic(shared_features)\n",
        "        return action_probs, state_value\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_probs, state_value = self.forward(state_tensor)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        return action.item(), log_prob, state_value\n",
        "\n",
        "# Create and test network\n",
        "actor_critic = ActorCritic(STATE_DIM, ACTION_DIM)\n",
        "test_state = env.reset()\n",
        "action, log_prob, value = actor_critic.get_action(test_state)\n",
        "print(f\"üß™ Actor-Critic Test:\")\n",
        "print(f\"   Selected Ad: {ADS[action]}\")\n",
        "print(f\"   State Value: {value.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìù Step 5: Define PPO Memory Buffer and Hyperparameters\n",
        "\n",
        "PPO collects experiences before learning. The memory stores states, actions, rewards, log probabilities, values, and done flags.\n",
        "\n",
        "### Key PPO Hyperparameters:\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| **gamma** | 0.99 | Discount factor for future rewards |\n",
        "| **epsilon** | 0.2 | Clipping range - THE KEY PPO INNOVATION! |\n",
        "| **Learning Rate** | 3e-4 | Step size for optimization |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Memory Buffer and Hyperparameters defined!\n",
            "   Clip Epsilon: 0.2 (policy can only change by ¬±20%)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PPO MEMORY BUFFER\n",
        "# ============================================================\n",
        "class PPOMemory:\n",
        "    def __init__(self):\n",
        "        self.clear()\n",
        "    \n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "    \n",
        "    def store(self, state, action, reward, log_prob, value, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "        self.dones.append(done)\n",
        "    \n",
        "    def get_batch(self):\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(self.states)),\n",
        "            torch.LongTensor(self.actions),\n",
        "            torch.FloatTensor(self.rewards),\n",
        "            torch.stack(self.log_probs).detach(),\n",
        "            torch.cat(self.values).detach(),\n",
        "            torch.FloatTensor(self.dones)\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "# ============================================================\n",
        "# PPO HYPERPARAMETERS\n",
        "# ============================================================\n",
        "GAMMA = 0.99          # Discount factor\n",
        "GAE_LAMBDA = 0.95     # GAE lambda parameter\n",
        "CLIP_EPSILON = 0.2    # THE KEY PPO CLIPPING PARAMETER!\n",
        "LEARNING_RATE = 3e-4  # Learning rate\n",
        "PPO_EPOCHS = 10       # Number of update epochs\n",
        "VALUE_COEF = 0.5      # Value loss coefficient\n",
        "ENTROPY_COEF = 0.01   # Entropy bonus coefficient\n",
        "TOTAL_TIMESTEPS = 10000\n",
        "UPDATE_INTERVAL = 256\n",
        "\n",
        "print(\"‚úÖ Memory Buffer and Hyperparameters defined!\")\n",
        "print(f\"   Clip Epsilon: {CLIP_EPSILON} (policy can only change by ¬±20%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Step 6: Define GAE and PPO Update Function\n",
        "\n",
        "### Generalized Advantage Estimation (GAE)\n",
        "Advantage tells us how much better an action was compared to average:\n",
        "$$A_t = Q(s_t, a_t) - V(s_t)$$\n",
        "\n",
        "### PPO Clipped Objective - THE CORE INNOVATION!\n",
        "$$L^{CLIP} = \\min(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t)$$\n",
        "\n",
        "Where $r_t(\\theta) = \\frac{\\pi_{new}(a|s)}{\\pi_{old}(a|s)}$ is the probability ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GAE and PPO Update functions defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPUTE GENERALIZED ADVANTAGE ESTIMATION (GAE)\n",
        "# ============================================================\n",
        "def compute_gae(rewards, values, dones, gamma=GAMMA, lam=GAE_LAMBDA):\n",
        "    \"\"\"Compute advantages using GAE for low-variance estimates\"\"\"\n",
        "    advantages = []\n",
        "    gae = 0\n",
        "    \n",
        "    for t in reversed(range(len(rewards))):\n",
        "        if t == len(rewards) - 1:\n",
        "            next_value = 0\n",
        "        else:\n",
        "            next_value = values[t + 1]\n",
        "        \n",
        "        if dones[t]:\n",
        "            next_value = 0\n",
        "            gae = 0\n",
        "        \n",
        "        # TD Error: delta = r + gamma*V(s') - V(s)\n",
        "        delta = rewards[t] + gamma * next_value - values[t]\n",
        "        \n",
        "        # GAE: A = delta + gamma*lambda*A_{t+1}\n",
        "        gae = delta + gamma * lam * gae\n",
        "        advantages.insert(0, gae)\n",
        "    \n",
        "    advantages = torch.FloatTensor(advantages)\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "    returns = advantages + torch.FloatTensor(values)\n",
        "    \n",
        "    return advantages, returns\n",
        "\n",
        "# ============================================================\n",
        "# PPO UPDATE FUNCTION - THE HEART OF PPO!\n",
        "# ============================================================\n",
        "def ppo_update(actor_critic, optimizer, memory, epochs=PPO_EPOCHS):\n",
        "    \"\"\"Perform PPO update with clipped objective\"\"\"\n",
        "    states, actions, rewards, old_log_probs, values, dones = memory.get_batch()\n",
        "    advantages, returns = compute_gae(rewards.tolist(), values.tolist(), dones.tolist())\n",
        "    \n",
        "    total_loss_info = {'policy_loss': 0, 'value_loss': 0, 'entropy': 0}\n",
        "    \n",
        "    for _ in range(epochs):\n",
        "        action_probs, state_values = actor_critic(states)\n",
        "        state_values = state_values.squeeze()\n",
        "        dist = Categorical(action_probs)\n",
        "        new_log_probs = dist.log_prob(actions)\n",
        "        entropy = dist.entropy().mean()\n",
        "        \n",
        "        # ============================================================\n",
        "        # PROBABILITY RATIO: r(theta) = pi_new / pi_old\n",
        "        # ============================================================\n",
        "        ratio = torch.exp(new_log_probs - old_log_probs.squeeze())\n",
        "        \n",
        "        # ============================================================\n",
        "        # CLIPPED OBJECTIVE - Prevents large policy updates!\n",
        "        # ============================================================\n",
        "        surr1 = ratio * advantages\n",
        "        surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantages\n",
        "        policy_loss = -torch.min(surr1, surr2).mean()\n",
        "        \n",
        "        # Value loss and total loss\n",
        "        value_loss = nn.MSELoss()(state_values, returns)\n",
        "        loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(actor_critic.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss_info['policy_loss'] += policy_loss.item()\n",
        "        total_loss_info['value_loss'] += value_loss.item()\n",
        "        total_loss_info['entropy'] += entropy.item()\n",
        "    \n",
        "    return {k: v/epochs for k, v in total_loss_info.items()}\n",
        "\n",
        "print(\"‚úÖ GAE and PPO Update functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Step 7: Train the PPO Agent\n",
        "\n",
        "Now we train our ad recommendation agent! The training loop:\n",
        "1. **Collect experiences**: Agent selects ads for users\n",
        "2. **Store in memory**: Record states, actions, rewards\n",
        "3. **Update policy**: Apply PPO update with clipping\n",
        "4. **Track progress**: Monitor CTR improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting PPO Training for Ad Optimization...\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for -: 'float' and 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m actor_critic, avg_rewards, policy_losses\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m trained_agent, rewards_history, loss_history = \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# PHASE 2: Update policy with PPO\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) >= UPDATE_INTERVAL:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     update_info = \u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     recent_ctr = np.mean(episode_rewards[-UPDATE_INTERVAL:])\n\u001b[32m     36\u001b[39m     avg_rewards.append(recent_ctr)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mppo_update\u001b[39m\u001b[34m(actor_critic, optimizer, memory, epochs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform PPO update with clipped objective\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m states, actions, rewards, old_log_probs, values, dones = memory.get_batch()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m advantages, returns = \u001b[43mcompute_gae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m total_loss_info = {\u001b[33m'\u001b[39m\u001b[33mpolicy_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m}\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mcompute_gae\u001b[39m\u001b[34m(rewards, values, dones, gamma, lam)\u001b[39m\n\u001b[32m     17\u001b[39m     gae = \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# TD Error: delta = r + gamma*V(s') - V(s)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m delta = \u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_value\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# GAE: A = delta + gamma*lambda*A_{t+1}\u001b[39;00m\n\u001b[32m     23\u001b[39m gae = delta + gamma * lam * gae\n",
            "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'float' and 'list'"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================\n",
        "def train_ppo():\n",
        "    env = AdvertisementEnvironment()\n",
        "    actor_critic = ActorCritic(STATE_DIM, ACTION_DIM)\n",
        "    optimizer = optim.Adam(actor_critic.parameters(), lr=LEARNING_RATE)\n",
        "    memory = PPOMemory()\n",
        "    \n",
        "    episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    policy_losses = []\n",
        "    \n",
        "    print(\"üöÄ Starting PPO Training for Ad Optimization...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    state = env.reset()\n",
        "    timestep = 0\n",
        "    \n",
        "    while timestep < TOTAL_TIMESTEPS:\n",
        "        # PHASE 1: Collect experiences\n",
        "        for _ in range(UPDATE_INTERVAL):\n",
        "            timestep += 1\n",
        "            action, log_prob, value = actor_critic.get_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            memory.store(state, action, reward, log_prob, value, done)\n",
        "            episode_rewards.append(reward)\n",
        "            state = next_state\n",
        "            if timestep >= TOTAL_TIMESTEPS:\n",
        "                break\n",
        "        \n",
        "        # PHASE 2: Update policy with PPO\n",
        "        if len(memory) >= UPDATE_INTERVAL:\n",
        "            update_info = ppo_update(actor_critic, optimizer, memory)\n",
        "            recent_ctr = np.mean(episode_rewards[-UPDATE_INTERVAL:])\n",
        "            avg_rewards.append(recent_ctr)\n",
        "            policy_losses.append(update_info['policy_loss'])\n",
        "            memory.clear()\n",
        "            \n",
        "            if timestep % 2000 == 0 or timestep == UPDATE_INTERVAL:\n",
        "                print(f\"Step {timestep}: CTR = {recent_ctr:.2%}\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "    print(\"‚úÖ Training Complete!\")\n",
        "    return actor_critic, avg_rewards, policy_losses\n",
        "\n",
        "# Train the agent\n",
        "trained_agent, rewards_history, loss_history = train_ppo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Step 8: Visualize Training Progress\n",
        "\n",
        "Let's see how our agent improved over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot CTR over time\n",
        "axes[0].plot(rewards_history, color='#2ecc71', linewidth=2)\n",
        "axes[0].axhline(y=0.25, color='red', linestyle='--', label='Random Policy (~25%)')\n",
        "axes[0].set_title('Click-Through Rate Over Training', fontweight='bold')\n",
        "axes[0].set_xlabel('Update Step')\n",
        "axes[0].set_ylabel('CTR')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Policy Loss\n",
        "axes[1].plot(loss_history, color='#e74c3c', linewidth=2)\n",
        "axes[1].set_title('Policy Loss Over Training', fontweight='bold')\n",
        "axes[1].set_xlabel('Update Step')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà Results Summary:\")\n",
        "print(f\"   Initial CTR: {rewards_history[0]:.2%}\")\n",
        "print(f\"   Final CTR: {rewards_history[-1]:.2%}\")\n",
        "print(f\"   Improvement: +{(rewards_history[-1] - rewards_history[0]):.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üß™ Step 9: Evaluate and Analyze Learned Policy\n",
        "\n",
        "Let's see what the agent learned about matching ads to user interests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUATE TRAINED AGENT\n",
        "# ============================================================\n",
        "def evaluate_agent(agent, n_episodes=1000):\n",
        "    env = AdvertisementEnvironment()\n",
        "    agent.eval()\n",
        "    \n",
        "    total_clicks = 0\n",
        "    interest_action_map = {interest: {ad: 0 for ad in ADS} for interest in INTERESTS}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_episodes):\n",
        "            state = env.reset()\n",
        "            interest = INTERESTS[env.current_user_info['interest']]\n",
        "            \n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action_probs, _ = agent(state_tensor)\n",
        "            action = action_probs.argmax().item()\n",
        "            \n",
        "            _, reward, _, _ = env.step(action)\n",
        "            total_clicks += reward\n",
        "            interest_action_map[interest][ADS[action]] += 1\n",
        "    \n",
        "    return total_clicks / n_episodes, interest_action_map\n",
        "\n",
        "ctr, interest_action_map = evaluate_agent(trained_agent)\n",
        "print(f\"üéØ Evaluation CTR: {ctr:.2%}\")\n",
        "\n",
        "# Create heatmap of learned policy\n",
        "print(\"\\nüìä Learned Policy Heatmap:\")\n",
        "heatmap_data = np.zeros((len(INTERESTS), len(ADS)))\n",
        "for i, interest in enumerate(INTERESTS):\n",
        "    total = sum(interest_action_map[interest].values())\n",
        "    if total > 0:\n",
        "        for j, ad in enumerate(ADS):\n",
        "            heatmap_data[i, j] = interest_action_map[interest][ad] / total\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "im = ax.imshow(heatmap_data, cmap='YlGn')\n",
        "ax.set_xticks(range(len(ADS)))\n",
        "ax.set_yticks(range(len(INTERESTS)))\n",
        "ax.set_xticklabels(ADS)\n",
        "ax.set_yticklabels(INTERESTS)\n",
        "\n",
        "for i in range(len(INTERESTS)):\n",
        "    for j in range(len(ADS)):\n",
        "        ax.text(j, i, f'{heatmap_data[i, j]:.0%}', ha='center', va='center', fontsize=12)\n",
        "\n",
        "ax.set_title('Learned Policy: Ad Selection by User Interest', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Ad Shown')\n",
        "ax.set_ylabel('User Interest')\n",
        "plt.colorbar(im, label='Selection Probability')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéì Step 10: Key Takeaways\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **PPO for Ads**: PPO can effectively learn to match ads to user interests\n",
        "\n",
        "2. **Clipping Mechanism**: The `min(ratio, clip(ratio))` ensures stable training by preventing large policy updates\n",
        "\n",
        "3. **Actor-Critic Architecture**: \n",
        "   - Actor learns the ad selection policy\n",
        "   - Critic reduces variance in training\n",
        "\n",
        "### PPO Formula Recap:\n",
        "$$L^{CLIP}(\\theta) = \\mathbb{E}_t[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
        "\n",
        "### Why PPO Works Well:\n",
        "- **Stable**: Clipping prevents \"going crazy\" during learning\n",
        "- **Efficient**: Reuses collected data multiple times\n",
        "- **Simple**: Easier to implement than TRPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üéâ PPO ADVERTISEMENT OPTIMIZATION - COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Performance:\")\n",
        "print(f\"   Random Policy CTR: ~25%\")\n",
        "print(f\"   Trained Agent CTR: {ctr:.1%}\")\n",
        "print(f\"   Relative Improvement: {(ctr - 0.25) / 0.25 * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüß† What the Agent Learned:\")\n",
        "print(f\"   Sports interest ‚Üí Sports_Ad\")\n",
        "print(f\"   Tech interest ‚Üí Tech_Ad\")\n",
        "print(f\"   Fashion interest ‚Üí Fashion_Ad\")\n",
        "print(f\"   Food interest ‚Üí Food_Ad\")\n",
        "\n",
        "print(f\"\\n‚ö° Key PPO Concepts Demonstrated:\")\n",
        "print(f\"   1. Clipped objective prevents large policy changes\")\n",
        "print(f\"   2. Actor-Critic architecture for stable learning\")\n",
        "print(f\"   3. GAE for low-variance advantage estimation\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ You've learned PPO for Ad Optimization!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
