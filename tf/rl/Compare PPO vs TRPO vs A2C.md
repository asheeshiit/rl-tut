# Compare PPO vs TRPO vs A2C

## 1. One-line Intuition

| Algorithm | Intuition |
|-----------|-----------|
| **A2C**   | "Update policy directly using advantage" |
| **TRPO**  | "Update policy safely using strict constraints" |
| **PPO**   | "Update policy safely using a simple penalty (clipping)" |

---

## 2. High-level Comparison Table

| Aspect | A2C | TRPO | PPO |
|--------|-----|------|-----|
| Type | Actorâ€“Critic | Policy Optimization | Policy Optimization |
| Stability | âŒ Lowâ€“Medium | âœ… Very High | âœ… High |
| Complexity | âœ… Simple | âŒ Very Complex | âœ… Simple |
| Sample Efficiency | âŒ Low | âœ… High | âœ… High |
| Implementation | Easy | Hard | Easy |
| Uses second-order info | âŒ No | âœ… Yes | âŒ No |
| On-policy | âœ… Yes | âœ… Yes | âœ… Yes |
| Industry usage | Medium | Low | â­ Very High |

---

## 3. A2C (Advantage Actor-Critic)

### Core Idea

- **Actor** updates policy using advantage
- **Critic** estimates value

### Objective

$$L_{A2C} = \mathbb{E} \left[ \log \pi(a \mid s) \cdot A(s, a) \right]$$

### Problem

- No restriction on update size
- Large gradient steps â†’ unstable learning

### When A2C Fails

- Continuous action spaces
- Long horizons
- High variance environments

---

## 4. TRPO (Trust Region Policy Optimization)

### Core Idea

> "Never change the policy too much in one step."

### Objective

$$\max_{\theta} \; \mathbb{E} \left[ r(\theta) \cdot A \right]$$

### Subject to Constraint

$$\mathbb{E} \left[ KL(\pi_{old} \| \pi_{\theta}) \right] \leq \delta$$

**Meaning:** Policy change must stay inside a *trust region*

### How It's Enforced

- KL-divergence constraint
- Conjugate gradient
- Fisher Information Matrix

### Pros

- âœ… Extremely stable
- âœ… Theoretically sound

### Cons

- âŒ Hard to implement
- âŒ Slow
- âŒ Computationally heavy

---

## 5. PPO (Proximal Policy Optimization)

### Core Idea

> "Approximate TRPO, but make it simple."

### Objective (Clipped)

$$L_{CLIP} = \mathbb{E} \left[ \min \left( r \cdot A, \; \text{clip}(r, 1-\epsilon, 1+\epsilon) \cdot A \right) \right]$$

Where:
- \( r = \frac{\pi_\theta(a|s)}{\pi_{old}(a|s)} \) â€” probability ratio
- \( A \) â€” advantage estimate
- \( \epsilon \) â€” clipping hyperparameter (typically 0.1â€“0.2)

### Key Difference from TRPO

| TRPO | PPO |
|------|-----|
| Hard KL constraint | Soft clipping |
| Complex math | Simple SGD |
| Second-order | First-order |

---

## 6. Mathematical Comparison (Side-by-Side)

| Component | A2C | TRPO | PPO |
|-----------|-----|------|-----|
| Policy ratio | âŒ No | âœ… Yes | âœ… Yes |
| Advantage | âœ… | âœ… | âœ… |
| Update constraint | âŒ None | âœ… KL | âœ… Clipping |
| Gradient type | First-order | Second-order | First-order |

---

## 7. Stability Intuition (Visual)

```
A2C:   ðŸš€  (big jumps â†’ unstable)
TRPO:  ðŸ§±  (hard wall â†’ safe but slow)
PPO:   ðŸ§¸  (soft padding â†’ safe & fast)
```

---

## 8. Performance Trade-offs

### A2C
- âœ… Fast per step
- âŒ Often diverges
- âŒ Sensitive to hyperparameters

### TRPO
- âœ… Very stable
- âŒ Hard to tune
- âŒ Expensive computation

### PPO
- âœ… Stable
- âœ… Efficient
- âœ… Easy to tune
- â­ **Best practical choice**

---

## 9. When to Use Which?

### Use A2C if:
- Simple environment
- Discrete actions
- Learning stability not critical

### Use TRPO if:
- Research / theory work
- You need guaranteed monotonic improvement
- Compute cost not an issue

### Use PPO if:
- Real-world problems
- Robotics
- Games
- RLHF
- You want best trade-off

---

## 10. Final Takeaway

| Algorithm | Summary |
|-----------|---------|
| A2C | Learns fast but **unstable** |
| TRPO | Learns safely but **expensively** |
| PPO | Learns safely and **efficiently** |

> â­ **Industry rule of thumb:**  
> *If unsure â†’ use PPO*
