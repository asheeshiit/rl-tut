# Conservative Q-Learning (CQL)

## What is CQL? (Layman's Terms)

Imagine you're trying to learn how to drive only from dashcam videos (a fixed dataset). You never get to practice driving yourself.

A "normal" RL algorithm might look at the videos and then confidently conclude:

> "Doing a crazy Uâ€‘turn at 90 mph must be awesome,"

simply because its internal value model (the Q-function) accidentally assigns a high score to that actionâ€”even though the dataset never shows anyone doing it.

**CQL's core idea is:**

> "If I haven't seen an action in the dataset (or something close to it), I should assume it's probably worse than I think, not better."

So CQL trains a Q-function that is **pessimistic / conservative** about actions that are **out-of-distribution (OOD)** relative to the dataset. This prevents the learned policy from exploiting "hallucinated" high Q-values.

This is especially important in **offline RL**, where you cannot test actions in the real environment to correct mistakes.

---

## Quick RL Refresher: What Q(s, a) Means

We assume an MDP with:
- State `s`
- Action `a`
- Reward `r`
- Discount `Î³ âˆˆ [0, 1)`

The **action-value function**:

```
Q(s, a) = ğ”¼[ Î£(t=0 to âˆ) Î³áµ— râ‚œ | sâ‚€ = s, aâ‚€ = a ]
```

In standard Q-learning, the "truth" satisfies the **Bellman optimality equation**:

```
Q*(s, a) = ğ”¼[ r + Î³ Â· max_a' Q*(s', a') ]
```

---

## Why Offline Q-Learning Breaks

In offline RL you only have a dataset:

```
ğ’Ÿ = { (s, a, r, s') }
```

collected by some behavior policy `Î²(a|s)` (unknown or implicit).

If you do standard Q-learning, you train with targets like:

```
y = r + Î³ Â· max_a' Q_Î¸Ì„(s', a')
```

**Problem:** The `max` might pick an action `a'` that *never appears in the dataset*.

Because neural nets + bootstrapping can overestimate, those unseen actions can get inflated values, and then the max keeps reinforcing them.

So the learned policy:

```
Ï€(s) = argmax_a Q(s, a)
```

may choose actions the dataset never supported.

---

## CQL: The Key "Conservatism" Penalty

CQL modifies training by adding a regularizer that:
1. **Pushes DOWN** Q-values for actions in general (especially ones not supported by data)
2. While **not pushing down** (or even relatively favoring) the dataset actions

### Discrete-Action Version (Cleanest to Understand)

CQL learns `Q_Î¸` by minimizing:

```
L(Î¸) = TD_Loss + Î± Â· CQL_Penalty

Where:
  TD_Loss    = ğ”¼_{(s,a,r,s') ~ ğ’Ÿ} [ (Q_Î¸(s,a) - y)Â² ]
  
  CQL_Penalty = ğ”¼_{s ~ ğ’Ÿ} [ log Î£_a exp(Q_Î¸(s,a)) - ğ”¼_{a ~ ğ’Ÿ(Â·|s)}[Q_Î¸(s,a)] ]
```

Where:
- `Î± > 0` controls how conservative you are
- `ğ’Ÿ(Â·|s)` is the empirical action distribution in the dataset at state `s`
- `y` is a TD target (often using a target network `Q_Î¸Ì„`)

---

## Understanding the Penalty (Intuition)

The term:

```
log Î£_a exp(Q(s, a))
```

is **log-sum-exp (LSE)**, a smooth approximation of `max_a Q(s, a)`, because:

```
max_a Q(s,a)  â‰¤  log Î£_a exp(Q(s,a))  â‰¤  max_a Q(s,a) + log|A|
```

So the penalty roughly behaves like:

```
(soft max over all actions) - (Q of dataset actions)
```

To minimize it, the model tends to:
- **Reduce** high Q-values for actions that are not dataset-supported
- **Keep** (relative) value on actions that appear in data

---

## The Math: How It Pushes Down Unseen Actions

### Gradient Insight

Define for a fixed state `s`:

```
LSE(s) = log Î£_a exp(Q(s, a))
```

A key derivative:

```
âˆ‚LSE(s)         exp(Q(s, a))
-------- = ---------------------- = softmax(Q(s, Â·))_a
âˆ‚Q(s, a)    Î£_a' exp(Q(s, a'))
```

So the CQL penalty gradient (for one state) looks like:

```
âˆ‚/âˆ‚Q(s,a) [ LSE(s) - ğ”¼_{a ~ ğ’Ÿ(Â·|s)} Q(s,a) ] = softmax_a - ğ’Ÿ(a|s)
```

### What This Means:

**If action `a` is NOT in the dataset** at `s`, then `ğ’Ÿ(a|s) â‰ˆ 0`
  - â†’ Gradient â‰ˆ `softmax_a > 0`
  - â†’ Gradient descent will **decrease** `Q(s, a)`

**If action `a` IS the dataset action** (say the dataset always took that action), `ğ’Ÿ(a|s) â‰ˆ 1`, and since `softmax_a < 1`, the gradient is negative
  - â†’ Gradient descent **increases** `Q(s, a)` (or at least decreases it less than others)

**That's the "conservative" mechanism in a nutshell.**

---

## CQL Algorithm (Practical Training Loop)

Here's the standard structure (discrete actions):

```
Given: offline dataset D of transitions (s, a, r, s')
Initialize Q-network Q_Î¸ and target Q_Î¸Ì„

repeat:
  Sample minibatch B from D

  # TD target (one common choice)
  y = r + Î³ Â· max_a' Q_Î¸Ì„(s', a')

  TD_loss = mean_{(s,a,r,s') in B} (Q_Î¸(s,a) - y)Â²

  CQL_loss = mean_{s in B} [ logsumexp_a Q_Î¸(s,a) - Q_Î¸(s, a_data) ]

  Total_loss = TD_loss + Î± Â· CQL_loss

  Î¸ â† Î¸ - Î· Â· âˆ‡_Î¸ Total_loss

  Periodically update target network: Î¸Ì„ â† Ï„Â·Î¸ + (1-Ï„)Â·Î¸Ì„

until done

Return policy Ï€(s) = argmax_a Q_Î¸(s,a)
```

For **continuous actions**, you can't sum over all actions, so CQL approximates the `log âˆ« exp(Q(s,a)) da` term via action samples (often integrated into a SAC-style actor-critic). The idea remains identical: penalize high Q on actions not supported by the dataset.

---

## Worked Numeric Example

*Shows the "push down OOD actions" effect*

### Setup

- One state `s`, three actions: `{aâ‚, aâ‚‚, aâ‚ƒ}`
- Dataset `ğ’Ÿ` only contains action `aâ‚` at this state (never `aâ‚‚` or `aâ‚ƒ`)

Suppose the current Q-values are:

| Action | Q-value | Status                 |
|--------|---------|------------------------|
| aâ‚     | 0.5     | Seen in data           |
| aâ‚‚     | 5.0     | Unseen, overestimated  |
| aâ‚ƒ     | 2.0     | Unseen                 |

If you greedily act, you'd choose `aâ‚‚` because it looks bestâ€”but it's not supported by data.

### Step 1: Compute CQL Penalty

Compute log-sum-exp:

```
LSE = log(e^0.5 + e^5 + e^2)
```

Numerically:
- `e^0.5 â‰ˆ 1.6487`
- `e^5 â‰ˆ 148.4132`
- `e^2 â‰ˆ 7.3891`
- Sum â‰ˆ 157.451
- `LSE â‰ˆ log(157.451) â‰ˆ 5.0591`

Dataset action is `aâ‚`, so subtract `Q(s, aâ‚) = 0.5`:

```
CQL penalty = 5.0591 - 0.5 = 4.5591
```

It's large because some action (here `aâ‚‚`) is extremely high.

### Step 2: Compute the Gradients (Important Part)

Softmax probabilities:

```
            e^Q(s,a)
p(a) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Î£_a' e^Q(s,a')
```

Numerically:

| Action | Softmax p(a) |
|--------|--------------|
| aâ‚     | â‰ˆ 0.01047    |
| aâ‚‚     | â‰ˆ 0.94260    |
| aâ‚ƒ     | â‰ˆ 0.04693    |

Gradient of `LSE - Q(s, aâ‚)` w.r.t each `Q(s, a)`:

| Action | Gradient            | Effect of Gradient Descent |
|--------|---------------------|---------------------------|
| aâ‚     | p(aâ‚) - 1 â‰ˆ -0.989  | **Increases** Q(s, aâ‚)    |
| aâ‚‚     | p(aâ‚‚) â‰ˆ +0.943      | **Decreases** Q(s, aâ‚‚)    |
| aâ‚ƒ     | p(aâ‚ƒ) â‰ˆ +0.047      | **Decreases** Q(s, aâ‚ƒ)    |

### Step 3: One Gradient Step (CQL Term Only)

Let learning rate `Î· = 0.1`, `Î± = 1`.

Update rule: `Q â† Q - Î· Â· âˆ‡`

| Action | Before | After                            |
|--------|--------|----------------------------------|
| aâ‚     | 0.5    | 0.5 - 0.1Ã—(-0.989) â‰ˆ **0.599**   |
| aâ‚‚     | 5.0    | 5.0 - 0.1Ã—(0.943) â‰ˆ **4.906**    |
| aâ‚ƒ     | 2.0    | 2.0 - 0.1Ã—(0.047) â‰ˆ **1.995**    |

### What Happened?

- The **overestimated unseen action** `aâ‚‚` got **pushed down**
- The **dataset action** `aâ‚` got **pushed up** (relative preference)

Repeat this over many batches/states and you stop the policy from preferring unseen actions.

---

## Combining with TD Learning

At the same time, the TD loss will push `Q(s, aâ‚)` toward the correct return based on real rewards in the dataset. So you end up with:

- **Dataset-supported actions** become accurately valued
- **Unsupported actions** become pessimistically low (unless evidence supports them)

---

## The Role of Î±: How Conservative is "Conservative"?

| Î± Value       | Effect                                                                                      |
|---------------|---------------------------------------------------------------------------------------------|
| **Too small** | Behaves closer to standard offline Q-learning â†’ risk of OOD overestimation                 |
| **Too large** | Becomes extremely pessimistic â†’ policy may stay very close to behavior (resembles BC)      |

> **Note:** Some implementations adapt `Î±` automatically using a constraint + Lagrangian method, but the concept above is the core.
