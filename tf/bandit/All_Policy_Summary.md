# ğŸ° TF-Agents Bandit Policies: Complete Reference Guide

## ğŸ“– Core Concepts

> **Exploitation**: Do what already works best  
> **Exploration**: Try uncertain options to learn more

Every policy below handles this tradeoff differently.
---

## ğŸ”¥ Quick Comparison Table

| Policy | Exploration | Context | Model Type | Best For | Key Strength | Key Weakness |
|--------|:-----------:|:-------:|------------|----------|--------------|--------------|
| **Bernoulli TS** | Probabilistic | âŒ | Bayesian | CTR optimization | Strong exploration | Binary only |
| **Boltzmann** | Soft | âŒ | Heuristic | Online experiments | Simple | Temp sensitive |
| **Categorical** | Learned | âœ… | Neural | RL agents | Flexible | Data hungry |
| **Falcon** | âŒ | âœ… | ML Model | Ads production | Stable | No learning |
| **Greedy MultiObj** | âŒ | âœ… | Neural | Business tradeoffs | Constraint-aware | Weight tuning |
| **Greedy RP** | âŒ | âŒ | Any | Baselines | Fast | Stagnates |
| **LinearUCB** | Confidence | âœ… | Linear | Contextual ads | Theory-backed | Linear assumption |
| **LinearBandit** | âŒ | âœ… | Linear | Simple systems | Interpretable | Weak exploration |
| **Linear TS** | Bayesian | âœ… | Linear | Contextual CTR | Excellent balance | Complex |
| **Mixture** | Configurable | Depends | Hybrid | Safe rollout | Robust | Hard to tune |
| **NeuralLinUCB** | Confidence | âœ… | Hybrid | Complex ads | Powerful | Heavy |
| **Ranking** | Depends | âœ… | ML | Search & Ads | UX aligned | Expensive |
| **RP Base** | âŒ | Depends | Any | Framework | Extensible | Not standalone |

---

## ğŸ¯ Decision Flowchart

```
START
  â”‚
  â”œâ”€ Binary rewards (click/no-click)?
  â”‚    â”œâ”€ YES + No context â†’ BernoulliThompsonSampling
  â”‚    â””â”€ YES + With context â†’ LinearThompsonSampling
  â”‚
  â”œâ”€ Need multiple objectives?
  â”‚    â””â”€ YES â†’ GreedyMultiObjectiveNeural
  â”‚
  â”œâ”€ Need ranking (not single action)?
  â”‚    â””â”€ YES â†’ RankingPolicy
  â”‚
  â”œâ”€ Have contextual features?
  â”‚    â”œâ”€ Linear relationship â†’ LinearUCB or LinearTS
  â”‚    â””â”€ Non-linear â†’ NeuralLinUCB
  â”‚
  â”œâ”€ Production system (no exploration needed)?
  â”‚    â””â”€ YES â†’ Falcon or GreedyRewardPrediction
  â”‚
  â””â”€ Want safe experimentation?
       â””â”€ YES â†’ MixturePolicy
```
---

## 1. BernoulliThompsonSamplingPolicy

### ğŸ’¡ Layman Idea
Each action is a coin (success/failure). You guess how biased each coin is, sample from that belief, and pick the best.

### ğŸ“Œ When to Use
- Rewards are binary (click / no click)
- You want probabilistic exploration

### ğŸ”¢ Math
```
Reward ~ Bernoulli(Î¸)
Prior: Î¸ ~ Beta(Î±, Î²)
Sample Î¸Ì‚ from Beta, choose action with max Î¸Ì‚
```

### ğŸ“ Example
- Ad A: 7 clicks / 10 views
- Ad B: 2 clicks / 3 views
- Sampling may still pick B sometimes â†’ exploration

| âœ… Pros | âŒ Cons |
|---------|---------|
| Excellent exploration | Binary rewards only |
| Simple | No context unless extended |
| Fast convergence | |

---

## 2. BoltzmannRewardPredictionPolicy (Softmax)

### ğŸ’¡ Layman Idea
Give probability to each action proportional to how good it looks. Better actions â†’ higher chance, but never 0.

### ğŸ“Œ When to Use
- You have reward predictions
- You want smooth exploration

### ğŸ”¢ Math
```
P(a) = exp(Q(a)/Ï„) / Î£ exp(Q(i)/Ï„)

Ï„ = temperature
  High Ï„ â†’ more exploration
  Low Ï„  â†’ greedy
```

### ğŸ“ Example
Action scores: `[10, 8, 3]` â†’ Still sometimes pick 8 or 3

| âœ… Pros | âŒ Cons |
|---------|---------|
| Simple | Sensitive to temperature |
| Tunable exploration | Can over-explore bad actions |

---

## 3. CategoricalPolicy

### ğŸ’¡ Layman Idea
Policy directly outputs probability distribution: "Pick A with 60%, B with 30%, C with 10%"

### ğŸ“Œ When to Use
- Policy-gradient methods
- Discrete action spaces

### ğŸ”¢ Math
```
Ï€(a|s) = Categorical(pâ‚, pâ‚‚, ..., pâ‚™)
```

### ğŸ“ Example
Neural network outputs logits â†’ softmax â†’ action

| âœ… Pros | âŒ Cons |
|---------|---------|
| Very flexible | Needs lots of data |
| Works with deep RL | Can be unstable |

---

## 4. FalconRewardPredictionPolicy

### ğŸ’¡ Layman Idea
Production-grade reward prediction + greedy choice. Used in large-scale ad systems.

### ğŸ“Œ When to Use
- High traffic
- Stable reward models

### ğŸ”¢ Math
```
a* = argmax RÌ‚(a|x)
```

### ğŸ“ Example
Pick ad with highest predicted revenue

| âœ… Pros | âŒ Cons |
|---------|---------|
| Scalable | No exploration |
| Deterministic | Can get stuck in local optima |
| Easy to debug | |

---

## 5. GreedyMultiObjectiveNeuralPolicy

### ğŸ’¡ Layman Idea
Optimize multiple goals at once: Revenue, User experience, Fairness

### ğŸ“Œ When to Use
- Trade-offs matter
- Business constraints exist

### ğŸ”¢ Math
```
R = wâ‚Â·Râ‚ + wâ‚‚Â·Râ‚‚ + ...
```

### ğŸ“ Example
70% revenue + 30% CTR

| âœ… Pros | âŒ Cons |
|---------|---------|
| Handles business constraints | Hard to tune weights |
| Flexible objectives | Greedy (no exploration) |

---

## 6. GreedyRewardPredictionPolicy

### ğŸ’¡ Layman Idea
Always pick best predicted reward

### ğŸ“Œ When to Use
- You already trust your model
- Low risk tolerance

### ğŸ”¢ Math
```
a* = argmax RÌ‚(a)
```

| âœ… Pros | âŒ Cons |
|---------|---------|
| Simple | Zero exploration |
| Fast | Can degrade over time |

---

## 7. LinearUCBPolicy

### ğŸ’¡ Layman Idea
"This option looks good AND I'm uncertain â†’ try it"

### ğŸ“Œ When to Use
- Contextual bandits
- Linear reward relationship

### ğŸ”¢ Math
```
a* = argmax [RÌ‚(a) + Î± Â· uncertainty]
```

### ğŸ“ Example
New ad with few samples but high uncertainty gets explored

| âœ… Pros | âŒ Cons |
|---------|---------|
| Principled exploration | Assumes linearity |
| Strong theoretical guarantees | Heavy matrix ops |

---

## 8. LinearBanditPolicy

### ğŸ’¡ Layman Idea
Reward = linear function of features. Choose action with highest expected reward.

### ğŸ”¢ Math
```
R = Î¸áµ€x
```

| âœ… Pros | âŒ Cons |
|---------|---------|
| Interpretable | No uncertainty handling |
| Efficient | Weak exploration |

---

## 9. LinearThompsonSamplingPolicy

### ğŸ’¡ Layman Idea
Thompson Sampling with context

### ğŸ“Œ When to Use
- Contextual bandits
- Binary or continuous rewards

### ğŸ”¢ Math
```
Sample Î¸ from posterior â†’ maximize Î¸áµ€x
```

| âœ… Pros | âŒ Cons |
|---------|---------|
| Better than LinearUCB in practice | More complex |
| Natural exploration | Posterior approximation needed |

---

## 10. MixturePolicy

### ğŸ’¡ Layman Idea
Combine multiple policies: "70% greedy + 30% exploratory"

### ğŸ“ Example
- 50% Thompson Sampling
- 50% Greedy

| âœ… Pros | âŒ Cons |
|---------|---------|
| Robust | Hard to optimize |
| Easy experimentation | Debugging complexity |

---

## 11. NeuralLinUCBPolicy

### ğŸ’¡ Layman Idea
Deep network learns features, Linear UCB on top for exploration

### ğŸ”¢ Math
```
Input â†’ NN â†’ feature vector â†’ LinUCB â†’ action
```

| âœ… Pros | âŒ Cons |
|---------|---------|
| Handles non-linearity | Heavy computation |
| Principled exploration | Complex tuning |

---

## 12. RankingPolicy

### ğŸ’¡ Layman Idea
Not one action â†’ ordered list. Optimize entire ranking.

### ğŸ“Œ When to Use
- Ads
- Recommendations
- Search results

### ğŸ”¢ Math
```
Listwise / pairwise loss (NDCG, MRR)
```

| âœ… Pros | âŒ Cons |
|---------|---------|
| Matches real UX | Complex training |
| Powerful | Expensive inference |

---

## 13. RewardPredictionBasePolicy

### ğŸ’¡ Layman Idea
Base class: predict reward â†’ choose. No exploration logic by default.

| âœ… Pros | âŒ Cons |
|---------|---------|
| Reusable | Needs extension |
| Clean architecture | Not a learning policy itself |

