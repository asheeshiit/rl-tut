{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ TF-Agents RewardPredictionBasePolicy for Advertisement Optimization\n",
        "\n",
        "This notebook demonstrates how to use **TF-Agents' RewardPredictionBasePolicy** for a **Multi-Armed Bandit** problem in the **Advertisement Domain**.\n",
        "\n",
        "## Business Problem\n",
        "An ad platform needs to decide **which ad to show** to each user to maximize **click-through rate (CTR)**.\n",
        "\n",
        "### Why Bandits for Ads?\n",
        "\n",
        "- **Exploration vs Exploitation**: We need to balance showing ads we know work well vs. trying new ads\n",
        "- **Contextual Decisions**: User features (age, interests, device) affect which ad works best\n",
        "- **Immediate Feedback**: We get reward (click/no-click) right after showing the ad\n",
        "\n",
        "### RL Framework Mapping:\n",
        "| Bandit Concept | Ads Domain Equivalent |\n",
        "|----------------|----------------------|\n",
        "| **Context** | User features (age, interests, device, time) |\n",
        "| **Arms/Actions** | Different ads to display (Sports, Tech, Fashion, Food) |\n",
        "| **Reward** | +1 for click, 0 for no click |\n",
        "| **Policy** | Strategy to select ads based on user context |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîë What is RewardPredictionBasePolicy?\n",
        "\n",
        "**RewardPredictionBasePolicy** is a base class in TF-Agents that:\n",
        "\n",
        "1. **Predicts rewards** for each action given the current context\n",
        "2. **Selects the action** with the highest predicted reward (greedy)\n",
        "3. **Can be extended** to add exploration strategies\n",
        "\n",
        "### Key Components:\n",
        "- **Reward Network**: Neural network that predicts expected reward for each action\n",
        "- **Action Selection**: Typically greedy (pick best) or epsilon-greedy (explore sometimes)\n",
        "- **Training**: Update the reward network based on observed rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üì¶ Step 1: Install and Import Required Libraries\n",
        "\n",
        "We need:\n",
        "- **TensorFlow**: Deep learning framework\n",
        "- **TF-Agents**: RL library with bandit policies\n",
        "- **NumPy**: Numerical computations\n",
        "- **Matplotlib**: Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install tf-agents tensorflow tensorflow-probability tf-keras numpy matplotlib\n",
        "\n",
        "# IMPORTANT: Set legacy Keras mode for TF-Agents compatibility\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-31 15:32:41.212070: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-31 15:32:41.213072: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-12-31 15:32:41.218955: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-12-31 15:32:41.235080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-12-31 15:32:41.261854: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-12-31 15:32:41.269257: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-12-31 15:32:41.289921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-31 15:32:42.673481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TensorFlow version: 2.17.1\n",
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "\n",
        "\n",
        "# TF-Agents imports\n",
        "from tf_agents.specs import tensor_spec, array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.bandits.policies import reward_prediction_base_policy\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.utils import common\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Step 2: Define the Advertisement Domain Configuration\n",
        "\n",
        "We create a simulated advertisement environment with:\n",
        "- **User Features (Context)**: Age group, interest category, device type, time of day\n",
        "- **Available Ads (Arms)**: 4 different ads (Sports, Tech, Fashion, Food)\n",
        "- **Click Probabilities**: Each user segment has different preferences for ads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Context Dimension: 15\n",
            "üé¨ Number of Actions (Ads): 4\n",
            "\n",
            "üë§ User Features:\n",
            "   Age Groups: ['18-25', '26-35', '36-50', '50+']\n",
            "   Interests: ['Sports', 'Tech', 'Fashion', 'Food']\n",
            "   Devices: ['Mobile', 'Desktop', 'Tablet']\n",
            "   Time Slots: ['Morning', 'Afternoon', 'Evening', 'Night']\n",
            "\n",
            "üì¢ Available Ads: ['Sports_Ad', 'Tech_Ad', 'Fashion_Ad', 'Food_Ad']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ADVERTISEMENT DOMAIN CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# User feature categories\n",
        "AGE_GROUPS = ['18-25', '26-35', '36-50', '50+']          # 4 categories\n",
        "INTERESTS = ['Sports', 'Tech', 'Fashion', 'Food']        # 4 categories  \n",
        "DEVICES = ['Mobile', 'Desktop', 'Tablet']                # 3 categories\n",
        "TIME_SLOTS = ['Morning', 'Afternoon', 'Evening', 'Night'] # 4 categories\n",
        "\n",
        "# Available ads (arms/actions)\n",
        "ADS = ['Sports_Ad', 'Tech_Ad', 'Fashion_Ad', 'Food_Ad']  # 4 actions\n",
        "\n",
        "# Context dimension = 4 + 4 + 3 + 4 = 15 (one-hot encoded)\n",
        "CONTEXT_DIM = len(AGE_GROUPS) + len(INTERESTS) + len(DEVICES) + len(TIME_SLOTS)\n",
        "NUM_ACTIONS = len(ADS)\n",
        "\n",
        "print(f\"üìã Context Dimension: {CONTEXT_DIM}\")\n",
        "print(f\"üé¨ Number of Actions (Ads): {NUM_ACTIONS}\")\n",
        "print(f\"\\nüë§ User Features:\")\n",
        "print(f\"   Age Groups: {AGE_GROUPS}\")\n",
        "print(f\"   Interests: {INTERESTS}\")\n",
        "print(f\"   Devices: {DEVICES}\")\n",
        "print(f\"   Time Slots: {TIME_SLOTS}\")\n",
        "print(f\"\\nüì¢ Available Ads: {ADS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üé≤ Step 3: Create the Click Probability Matrix\n",
        "\n",
        "This function defines the **true click probabilities** for each user segment and ad combination.\n",
        "In real scenarios, this is unknown - we learn it through interaction!\n",
        "\n",
        "**Key Insight**: Users with matching interests have higher click probability:\n",
        "- Sports enthusiasts ‚Üí Sports_Ad\n",
        "- Tech enthusiasts ‚Üí Tech_Ad\n",
        "- etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Click probability function defined!\n",
            "\n",
            "üìà Example probabilities:\n",
            "   Sports fan + Sports_Ad: 65.00%\n",
            "   Sports fan + Tech_Ad: 30.00%\n",
            "   Sports fan + Fashion_Ad: 30.00%\n",
            "   Sports fan + Food_Ad: 30.00%\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CLICK PROBABILITY FUNCTION (Ground Truth - Unknown to Agent)\n",
        "# ============================================================\n",
        "\n",
        "def get_click_probability(context: np.ndarray, action: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate click probability based on user context and ad shown.\n",
        "    \n",
        "    Context format (one-hot encoded):\n",
        "    - [0:4]  = Age group\n",
        "    - [4:8]  = Interest\n",
        "    - [8:11] = Device\n",
        "    - [11:15] = Time slot\n",
        "    \n",
        "    Higher probability when ad matches user interest.\n",
        "    \"\"\"\n",
        "    # Extract interest from context (indices 4-8)\n",
        "    interest_idx = np.argmax(context[4:8])\n",
        "    \n",
        "    # Base click probability\n",
        "    base_prob = 0.1\n",
        "    \n",
        "    # Bonus if ad matches interest (interest_idx == action since both are aligned)\n",
        "    if interest_idx == action:\n",
        "        match_bonus = 0.4  # High bonus for matching\n",
        "    else:\n",
        "        match_bonus = 0.05  # Small bonus for non-matching\n",
        "    \n",
        "    # Age group effect (younger users click more on mobile)\n",
        "    age_idx = np.argmax(context[0:4])\n",
        "    device_idx = np.argmax(context[8:11])\n",
        "    \n",
        "    if age_idx < 2 and device_idx == 0:  # Young + Mobile\n",
        "        age_device_bonus = 0.1\n",
        "    else:\n",
        "        age_device_bonus = 0.0\n",
        "    \n",
        "    # Time effect (evening has higher engagement)\n",
        "    time_idx = np.argmax(context[11:15])\n",
        "    if time_idx == 2:  # Evening\n",
        "        time_bonus = 0.05\n",
        "    else:\n",
        "        time_bonus = 0.0\n",
        "    \n",
        "    # Combine all factors\n",
        "    final_prob = min(base_prob + match_bonus + age_device_bonus + time_bonus, 0.9)\n",
        "    \n",
        "    return final_prob\n",
        "\n",
        "print(\"‚úÖ Click probability function defined!\")\n",
        "print(\"\\nüìà Example probabilities:\")\n",
        "\n",
        "# Example: Young user interested in Sports on Mobile in Evening\n",
        "example_context = np.zeros(CONTEXT_DIM, dtype=np.float32)\n",
        "example_context[0] = 1.0   # Age: 18-25\n",
        "example_context[4] = 1.0   # Interest: Sports\n",
        "example_context[8] = 1.0   # Device: Mobile\n",
        "example_context[13] = 1.0  # Time: Evening\n",
        "\n",
        "\n",
        "for i, ad in enumerate(ADS):\n",
        "    prob = get_click_probability(example_context, i)\n",
        "    print(f\"   Sports fan + {ad}: {prob:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üè≠ Step 4: Create the Advertisement Bandit Environment\n",
        "\n",
        "We create a contextual bandit environment that:\n",
        "1. **Generates user contexts** (random user features)\n",
        "2. **Returns rewards** based on ad shown (click = 1, no click = 0)\n",
        "\n",
        "### Key Difference from RL:\n",
        "- **No state transitions**: Each user visit is independent\n",
        "- **Immediate reward**: We know right away if user clicked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advertisement Bandit Environment created!\n",
            "\n",
            "üìã Observation Spec: TensorSpec(shape=(15,), dtype=tf.float32, name='observation')\n",
            "üé¨ Action Spec: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32))\n",
            "‚è∞ Time Step Spec: TimeStep(\n",
            "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
            " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
            " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
            " 'observation': TensorSpec(shape=(15,), dtype=tf.float32, name='observation')})\n"
          ]
        }
      ],
      "source": [
        "class AdvertisementBanditEnvironment:\n",
        "    \"\"\"\n",
        "    Contextual Bandit Environment for Advertisement Selection.\n",
        "    \n",
        "    Each step:\n",
        "    1. A new user arrives with random features (context)\n",
        "    2. Agent selects an ad to show (action)\n",
        "    3. User clicks or not (reward: 1 or 0)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Define specs\n",
        "        self._observation_spec = tensor_spec.TensorSpec(\n",
        "            shape=(CONTEXT_DIM,), \n",
        "            dtype=tf.float32, \n",
        "            name='observation'\n",
        "        )\n",
        "        \n",
        "        self._action_spec = tensor_spec.BoundedTensorSpec(\n",
        "            shape=(), \n",
        "            dtype=tf.int32, \n",
        "            minimum=0, \n",
        "            maximum=NUM_ACTIONS - 1, \n",
        "            name='action'\n",
        "        )\n",
        "        \n",
        "        self._current_context = None\n",
        "        \n",
        "    @property\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "    \n",
        "    @property\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "    \n",
        "    @property\n",
        "    def time_step_spec(self):\n",
        "        \"\"\"Returns the time_step_spec for this environment.\"\"\"\n",
        "        return ts.time_step_spec(observation_spec=self._observation_spec)\n",
        "    \n",
        "    def _generate_random_context(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate a random user context (one-hot encoded features).\n",
        "        \"\"\"\n",
        "        context = np.zeros(CONTEXT_DIM, dtype=np.float32)\n",
        "        \n",
        "        # Randomly select one category for each feature\n",
        "        age_idx = np.random.randint(0, len(AGE_GROUPS))\n",
        "        interest_idx = np.random.randint(0, len(INTERESTS))\n",
        "        device_idx = np.random.randint(0, len(DEVICES))\n",
        "        time_idx = np.random.randint(0, len(TIME_SLOTS))\n",
        "        \n",
        "        # One-hot encode\n",
        "        context[age_idx] = 1.0\n",
        "        context[len(AGE_GROUPS) + interest_idx] = 1.0\n",
        "        context[len(AGE_GROUPS) + len(INTERESTS) + device_idx] = 1.0\n",
        "        context[len(AGE_GROUPS) + len(INTERESTS) + len(DEVICES) + time_idx] = 1.0\n",
        "        \n",
        "        return context\n",
        "    \n",
        "    def reset(self) -> ts.TimeStep:\n",
        "        \"\"\"\n",
        "        Reset environment and return initial time step with new user context.\n",
        "        \"\"\"\n",
        "        self._current_context = self._generate_random_context()\n",
        "        return ts.restart(tf.constant([self._current_context], dtype=tf.float32))\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[ts.TimeStep, float]:\n",
        "        \"\"\"\n",
        "        Execute action (show ad) and return reward (click/no-click).\n",
        "        \n",
        "        Args:\n",
        "            action: Index of ad to show\n",
        "            \n",
        "        Returns:\n",
        "            time_step: New time step with next user context\n",
        "            reward: 1.0 if clicked, 0.0 if not\n",
        "        \"\"\"\n",
        "        # Get click probability for this context-action pair\n",
        "        click_prob = get_click_probability(self._current_context, action)\n",
        "        \n",
        "        # Sample reward (Bernoulli with click_prob)\n",
        "        reward = 1.0 if np.random.random() < click_prob else 0.0\n",
        "        \n",
        "        # Generate new user context for next step\n",
        "        self._current_context = self._generate_random_context()\n",
        "        \n",
        "        # Return transition to new context\n",
        "        next_time_step = ts.transition(\n",
        "            observation=tf.constant([self._current_context], dtype=tf.float32),\n",
        "            reward=tf.constant([reward], dtype=tf.float32)\n",
        "        )\n",
        "        \n",
        "        return next_time_step, reward\n",
        "\n",
        "# Create environment instance\n",
        "env = AdvertisementBanditEnvironment()\n",
        "\n",
        "print(\"‚úÖ Advertisement Bandit Environment created!\")\n",
        "print(f\"\\nüìã Observation Spec: {env.observation_spec}\")\n",
        "print(f\"üé¨ Action Spec: {env.action_spec}\")\n",
        "print(f\"‚è∞ Time Step Spec: {env.time_step_spec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üß† Step 5: Create the Reward Prediction Network\n",
        "\n",
        "The **Reward Prediction Network** is a neural network that:\n",
        "1. Takes **context (user features)** as input\n",
        "2. Outputs **predicted reward for each action (ad)**\n",
        "\n",
        "### Architecture:\n",
        "```\n",
        "Context (15) ‚Üí Dense(64) ‚Üí ReLU ‚Üí Dense(32) ‚Üí ReLU ‚Üí Dense(4) ‚Üí Predicted Rewards\n",
        "```\n",
        "\n",
        "This network learns the mapping: `f(context) ‚Üí [reward_ad1, reward_ad2, reward_ad3, reward_ad4]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RewardPredictionNetwork(network.Network):\n",
        "    \"\"\"\n",
        "    Neural network that predicts expected reward for each action given context.\n",
        "    \n",
        "    Input: User context (one-hot encoded features)\n",
        "    Output: Predicted reward for each ad (NUM_ACTIONS outputs)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 observation_spec, \n",
        "                 action_spec,\n",
        "                 fc_layer_params=(64, 32),\n",
        "                 name='RewardPredictionNetwork'):\n",
        "        \"\"\"\n",
        "        Initialize the reward prediction network.\n",
        "        \n",
        "        Args:\n",
        "            observation_spec: Spec for observations (context)\n",
        "            action_spec: Spec for actions\n",
        "            fc_layer_params: Tuple of hidden layer sizes\n",
        "            name: Network name\n",
        "        \"\"\"\n",
        "        super(RewardPredictionNetwork, self).__init__(\n",
        "            input_tensor_spec=observation_spec,\n",
        "            state_spec=(),\n",
        "            name=name\n",
        "        )\n",
        "        \n",
        "        self._action_spec = action_spec\n",
        "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
        "        \n",
        "        # Build the network layers\n",
        "        self._layers = []\n",
        "        \n",
        "        # Hidden layers\n",
        "        for units in fc_layer_params:\n",
        "            self._layers.append(\n",
        "                tf.keras.layers.Dense(\n",
        "                    units,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        # Output layer: one output per action (predicted reward)\n",
        "        self._output_layer = tf.keras.layers.Dense(\n",
        "            self._num_actions,\n",
        "            activation='sigmoid',  # Rewards are between 0 and 1 (click probability)\n",
        "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
        "            name='reward_predictions'\n",
        "        )\n",
        "        \n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            observation: User context tensor [batch_size, CONTEXT_DIM]\n",
        "            step_type: Not used in bandits\n",
        "            network_state: Not used (stateless network)\n",
        "            training: Whether in training mode\n",
        "            \n",
        "        Returns:\n",
        "            predicted_rewards: Tensor of shape [batch_size, NUM_ACTIONS]\n",
        "            network_state: Empty tuple (stateless)\n",
        "        \"\"\"\n",
        "        x = observation\n",
        "        \n",
        "        # Pass through hidden layers\n",
        "        for layer in self._layers:\n",
        "            x = layer(x, training=training)\n",
        "        \n",
        "        # Output layer\n",
        "        predicted_rewards = self._output_layer(x, training=training)\n",
        "        \n",
        "        return predicted_rewards, network_state\n",
        "\n",
        "# Create the reward prediction network\n",
        "reward_network = RewardPredictionNetwork(\n",
        "    observation_spec=env.observation_spec,\n",
        "    action_spec=env.action_spec,\n",
        "    fc_layer_params=(64, 32)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Reward Prediction Network created!\")\n",
        "print(f\"\\nüß† Network Architecture:\")\n",
        "print(f\"   Input: Context ({CONTEXT_DIM} features)\")\n",
        "print(f\"   Hidden: Dense(64) ‚Üí ReLU ‚Üí Dense(32) ‚Üí ReLU\")\n",
        "print(f\"   Output: {NUM_ACTIONS} predicted rewards (one per ad)\")\n",
        "\n",
        "# Test the network\n",
        "test_time_step = env.reset()\n",
        "test_predictions, _ = reward_network(test_time_step.observation)\n",
        "print(f\"\\nüß™ Test prediction shape: {test_predictions.shape}\")\n",
        "print(f\"   Sample predictions: {test_predictions.numpy()[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Step 6: Create the RewardPredictionBasePolicy\n",
        "\n",
        "Now we create a policy that uses the reward prediction network to select actions.\n",
        "\n",
        "### How It Works:\n",
        "1. **Receive context** (user features)\n",
        "2. **Predict rewards** for all actions using the network\n",
        "3. **Select action** with highest predicted reward (greedy)\n",
        "4. **Optional**: Add epsilon-greedy exploration\n",
        "\n",
        "### Key Methods to Implement:\n",
        "- `_predict_rewards()`: Returns predicted rewards for all actions\n",
        "- `_distribution()`: Returns action distribution based on predicted rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdRewardPredictionPolicy(reward_prediction_base_policy.RewardPredictionBasePolicy):\n",
        "    \"\"\"\n",
        "    Policy that selects ads based on predicted click rewards.\n",
        "    \n",
        "    Uses epsilon-greedy exploration:\n",
        "    - With probability (1-epsilon): Select ad with highest predicted reward\n",
        "    - With probability epsilon: Select random ad\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 time_step_spec,\n",
        "                 action_spec,\n",
        "                 reward_network,\n",
        "                 epsilon=0.1,\n",
        "                 name='AdRewardPredictionPolicy'):\n",
        "        \"\"\"\n",
        "        Initialize the policy.\n",
        "        \n",
        "        Args:\n",
        "            time_step_spec: Spec for time steps\n",
        "            action_spec: Spec for actions\n",
        "            reward_network: Network that predicts rewards\n",
        "            epsilon: Exploration rate (probability of random action)\n",
        "            name: Policy name\n",
        "        \"\"\"\n",
        "        super(AdRewardPredictionPolicy, self).__init__(\n",
        "            time_step_spec=time_step_spec,\n",
        "            action_spec=action_spec,\n",
        "            name=name\n",
        "        )\n",
        "        \n",
        "        self._reward_network = reward_network\n",
        "        self._epsilon = epsilon\n",
        "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
        "        \n",
        "    @property\n",
        "    def reward_network(self):\n",
        "        return self._reward_network\n",
        "        \n",
        "    def _predict_rewards(self, time_step, policy_state):\n",
        "        \"\"\"\n",
        "        Predict rewards for all actions given current context.\n",
        "        \n",
        "        This is a KEY method that RewardPredictionBasePolicy expects!\n",
        "        \n",
        "        Args:\n",
        "            time_step: Current time step with observation\n",
        "            policy_state: Not used (stateless policy)\n",
        "            \n",
        "        Returns:\n",
        "            predicted_rewards: Tensor [batch_size, num_actions]\n",
        "        \"\"\"\n",
        "        predicted_rewards, _ = self._reward_network(\n",
        "            time_step.observation,\n",
        "            training=False\n",
        "        )\n",
        "        return predicted_rewards\n",
        "    \n",
        "    def _distribution(self, time_step, policy_state):\n",
        "        \"\"\"\n",
        "        Get action distribution based on predicted rewards.\n",
        "        \n",
        "        Uses epsilon-greedy: mostly greedy, sometimes random.\n",
        "        \n",
        "        Args:\n",
        "            time_step: Current time step\n",
        "            policy_state: Not used\n",
        "            \n",
        "        Returns:\n",
        "            PolicyStep with action distribution\n",
        "        \"\"\"\n",
        "        # Get predicted rewards for all actions\n",
        "        predicted_rewards = self._predict_rewards(time_step, policy_state)\n",
        "        \n",
        "        # Get batch size\n",
        "        batch_size = tf.shape(predicted_rewards)[0]\n",
        "        \n",
        "        # Greedy action (highest predicted reward)\n",
        "        greedy_action = tf.argmax(predicted_rewards, axis=1, output_type=tf.int32)\n",
        "        \n",
        "        # Random action for exploration\n",
        "        random_action = tf.random.uniform(\n",
        "            shape=(batch_size,),\n",
        "            minval=0,\n",
        "            maxval=self._num_actions,\n",
        "            dtype=tf.int32\n",
        "        )\n",
        "        \n",
        "        # Epsilon-greedy: choose random with probability epsilon\n",
        "        explore = tf.random.uniform(shape=(batch_size,)) < self._epsilon\n",
        "        action = tf.where(explore, random_action, greedy_action)\n",
        "        \n",
        "        # Return deterministic distribution at selected action\n",
        "        return policy_step.PolicyStep(\n",
        "            action=tfp.distributions.Deterministic(loc=action),\n",
        "            state=policy_state,\n",
        "            info={'predicted_rewards': predicted_rewards}\n",
        "        )\n",
        "\n",
        "# Create the policy\n",
        "policy = AdRewardPredictionPolicy(\n",
        "    time_step_spec=env.time_step_spec,\n",
        "    action_spec=env.action_spec,\n",
        "    reward_network=reward_network,\n",
        "    epsilon=0.1  # 10% exploration\n",
        ")\n",
        "\n",
        "print(\"‚úÖ AdRewardPredictionPolicy created!\")\n",
        "print(f\"\\nüé≤ Policy Configuration:\")\n",
        "print(f\"   Exploration rate (epsilon): {policy._epsilon}\")\n",
        "print(f\"   Number of actions: {policy._num_actions}\")\n",
        "\n",
        "# Test the policy\n",
        "test_time_step = env.reset()\n",
        "test_action_step = policy.action(test_time_step)\n",
        "print(f\"\\nüß™ Test action: {test_action_step.action.numpy()[0]} ({ADS[test_action_step.action.numpy()[0]]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üèãÔ∏è Step 7: Create the Training Loop\n",
        "\n",
        "Now we train the reward prediction network using observed data.\n",
        "\n",
        "### Training Process:\n",
        "1. **Collect experience**: Show ads, observe clicks\n",
        "2. **Update network**: Train to predict observed rewards\n",
        "3. **Repeat**: Continuously improve predictions\n",
        "\n",
        "### Loss Function:\n",
        "We use **Mean Squared Error (MSE)** between:\n",
        "- Predicted reward for the chosen action\n",
        "- Actual observed reward (0 or 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BanditTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for the reward prediction bandit policy.\n",
        "    Collects experience and updates the reward network.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, policy, learning_rate=0.001):\n",
        "        self.policy = policy\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.experience_buffer = []\n",
        "        self.buffer_size = 1000\n",
        "        self.rewards_history = []\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def collect_experience(self, env, num_steps=100):\n",
        "        \"\"\"Collect experience by interacting with environment.\"\"\"\n",
        "        total_reward = 0\n",
        "        \n",
        "        for _ in range(num_steps):\n",
        "            time_step = env.reset()\n",
        "            action_step = self.policy.action(time_step)\n",
        "            action = action_step.action.numpy()[0]\n",
        "            _, reward = env.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            experience = {\n",
        "                'context': time_step.observation.numpy()[0],\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "            }\n",
        "            self.experience_buffer.append(experience)\n",
        "            \n",
        "            if len(self.experience_buffer) > self.buffer_size:\n",
        "                self.experience_buffer.pop(0)\n",
        "        \n",
        "        avg_reward = total_reward / num_steps\n",
        "        self.rewards_history.append(avg_reward)\n",
        "        return avg_reward\n",
        "    \n",
        "    def train_step(self, batch_size=32):\n",
        "        \"\"\"Perform one training step on the reward network.\"\"\"\n",
        "        if len(self.experience_buffer) < batch_size:\n",
        "            return 0.0\n",
        "        \n",
        "        indices = np.random.choice(len(self.experience_buffer), batch_size, replace=False)\n",
        "        batch = [self.experience_buffer[i] for i in indices]\n",
        "        \n",
        "        contexts = tf.constant([exp['context'] for exp in batch], dtype=tf.float32)\n",
        "        actions = tf.constant([exp['action'] for exp in batch], dtype=tf.int32)\n",
        "        rewards = tf.constant([exp['reward'] for exp in batch], dtype=tf.float32)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted_rewards, _ = self.policy.reward_network(contexts, training=True)\n",
        "            batch_indices = tf.range(batch_size)\n",
        "            indices_2d = tf.stack([batch_indices, actions], axis=1)\n",
        "            predicted_for_action = tf.gather_nd(predicted_rewards, indices_2d)\n",
        "            loss = tf.reduce_mean(tf.square(predicted_for_action - rewards))\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.policy.reward_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.policy.reward_network.trainable_variables)\n",
        "        )\n",
        "        \n",
        "        self.loss_history.append(loss.numpy())\n",
        "        return loss.numpy()\n",
        "\n",
        "trainer = BanditTrainer(policy, learning_rate=0.001)\n",
        "print(\"‚úÖ BanditTrainer created!\")\n",
        "print(f\"\\nüìä Trainer Configuration:\")\n",
        "print(f\"   Learning rate: 0.001\")\n",
        "print(f\"   Buffer size: {trainer.buffer_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Step 8: Run the Training\n",
        "\n",
        "Now we train the bandit agent to learn optimal ad selection!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "NUM_ITERATIONS = 100\n",
        "COLLECT_STEPS = 50\n",
        "TRAIN_STEPS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(\"üöÄ Starting Training...\")\n",
        "print(f\"   Iterations: {NUM_ITERATIONS}\")\n",
        "print(f\"   Collect steps per iteration: {COLLECT_STEPS}\")\n",
        "print(f\"   Train steps per iteration: {TRAIN_STEPS}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    avg_reward = trainer.collect_experience(env, num_steps=COLLECT_STEPS)\n",
        "    \n",
        "    total_loss = 0\n",
        "    for _ in range(TRAIN_STEPS):\n",
        "        loss = trainer.train_step(batch_size=BATCH_SIZE)\n",
        "        total_loss += loss\n",
        "    avg_loss = total_loss / TRAIN_STEPS\n",
        "    \n",
        "    if (iteration + 1) % 10 == 0:\n",
        "        print(f\"üìà Iteration {iteration + 1:3d}: Avg Reward = {avg_reward:.4f}, Avg Loss = {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Training Complete!\")\n",
        "print(f\"\\nüìä Final Statistics:\")\n",
        "print(f\"   Final Average Reward: {trainer.rewards_history[-1]:.4f}\")\n",
        "print(f\"   Best Average Reward: {max(trainer.rewards_history):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìà Step 9: Visualize Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Reward History\n",
        "ax1 = axes[0]\n",
        "ax1.plot(trainer.rewards_history, color='#2ecc71', linewidth=2, alpha=0.8)\n",
        "ax1.axhline(y=0.25, color='#e74c3c', linestyle='--', label='Random Policy', alpha=0.7)\n",
        "\n",
        "window = 10\n",
        "if len(trainer.rewards_history) >= window:\n",
        "    smoothed = np.convolve(trainer.rewards_history, np.ones(window)/window, mode='valid')\n",
        "    ax1.plot(range(window-1, len(trainer.rewards_history)), smoothed, \n",
        "             color='#27ae60', linewidth=3, label='Smoothed')\n",
        "\n",
        "ax1.set_xlabel('Iteration', fontsize=12)\n",
        "ax1.set_ylabel('Average Reward (CTR)', fontsize=12)\n",
        "ax1.set_title('üéØ Click-Through Rate Over Training', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Loss History\n",
        "ax2 = axes[1]\n",
        "ax2.plot(trainer.loss_history, color='#3498db', linewidth=1, alpha=0.5)\n",
        "\n",
        "if len(trainer.loss_history) >= window:\n",
        "    smoothed_loss = np.convolve(trainer.loss_history, np.ones(window)/window, mode='valid')\n",
        "    ax2.plot(range(window-1, len(trainer.loss_history)), smoothed_loss,\n",
        "             color='#2980b9', linewidth=2, label='Smoothed')\n",
        "\n",
        "ax2.set_xlabel('Training Step', fontsize=12)\n",
        "ax2.set_ylabel('MSE Loss', fontsize=12)\n",
        "ax2.set_title('üìâ Training Loss', fontsize=14, fontweight='bold')\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîç Step 10: Evaluate the Learned Policy\n",
        "\n",
        "Let's see what the policy learned by checking predicted rewards for different user segments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_test_context(age_idx, interest_idx, device_idx, time_idx):\n",
        "    \"\"\"Create a specific user context for testing.\"\"\"\n",
        "    context = np.zeros(CONTEXT_DIM, dtype=np.float32)\n",
        "    context[age_idx] = 1.0\n",
        "    context[len(AGE_GROUPS) + interest_idx] = 1.0\n",
        "    context[len(AGE_GROUPS) + len(INTERESTS) + device_idx] = 1.0\n",
        "    context[len(AGE_GROUPS) + len(INTERESTS) + len(DEVICES) + time_idx] = 1.0\n",
        "    return context\n",
        "\n",
        "print(\"üîç Policy Evaluation: Predicted Rewards by User Interest\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for interest_idx, interest in enumerate(INTERESTS):\n",
        "    context = create_test_context(age_idx=1, interest_idx=interest_idx, device_idx=0, time_idx=2)\n",
        "    context_tensor = tf.constant([context], dtype=tf.float32)\n",
        "    predictions, _ = reward_network(context_tensor)\n",
        "    predictions = predictions.numpy()[0]\n",
        "    best_action = np.argmax(predictions)\n",
        "    \n",
        "    print(f\"\\nüë§ User interested in: {interest}\")\n",
        "    print(f\"   Predicted rewards: \", end=\"\")\n",
        "    for ad_idx, ad in enumerate(ADS):\n",
        "        marker = \"‚≠ê\" if ad_idx == best_action else \"  \"\n",
        "        print(f\"{ad}: {predictions[ad_idx]:.3f}{marker}  \", end=\"\")\n",
        "    print(f\"\\n   ‚Üí Selected: {ADS[best_action]} | Optimal: {ADS[interest_idx]} | {'‚úÖ' if best_action == interest_idx else '‚ùå'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with baseline policies\n",
        "def evaluate_policy(policy_fn, env, num_episodes=500):\n",
        "    \"\"\"Evaluate a policy over multiple episodes.\"\"\"\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = env.reset()\n",
        "        action = policy_fn(time_step.observation.numpy()[0])\n",
        "        _, reward = env.step(action)\n",
        "        total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "def random_policy(context):\n",
        "    return np.random.randint(0, NUM_ACTIONS)\n",
        "\n",
        "def optimal_policy(context):\n",
        "    probs = [get_click_probability(context, a) for a in range(NUM_ACTIONS)]\n",
        "    return np.argmax(probs)\n",
        "\n",
        "def learned_policy(context):\n",
        "    context_tensor = tf.constant([context], dtype=tf.float32)\n",
        "    predictions, _ = reward_network(context_tensor)\n",
        "    return tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "print(\"üìä Policy Comparison (500 episodes each)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "random_reward = evaluate_policy(random_policy, env)\n",
        "optimal_reward = evaluate_policy(optimal_policy, env)\n",
        "learned_reward = evaluate_policy(learned_policy, env)\n",
        "\n",
        "print(f\"\\nüé≤ Random Policy:   {random_reward:.4f} CTR\")\n",
        "print(f\"üß† Learned Policy:  {learned_reward:.4f} CTR\")\n",
        "print(f\"üèÜ Optimal Policy:  {optimal_reward:.4f} CTR\")\n",
        "\n",
        "improvement = (learned_reward - random_reward) / random_reward * 100\n",
        "print(f\"\\nüìà Improvement over random: +{improvement:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìù Summary: Key Steps Explained\n",
        "\n",
        "### Step-by-Step Breakdown:\n",
        "\n",
        "| Step | What We Did | Why It Matters |\n",
        "|------|-------------|----------------|\n",
        "| **1. Setup** | Imported TF-Agents and defined domain | Foundation for bandit implementation |\n",
        "| **2. Environment** | Created `AdvertisementBanditEnvironment` | Simulates user arrivals and clicks |\n",
        "| **3. Network** | Built `RewardPredictionNetwork` | Learns context ‚Üí reward mapping |\n",
        "| **4. Policy** | Implemented `AdRewardPredictionPolicy` | Extends RewardPredictionBasePolicy for ads |\n",
        "| **5. Training** | Created training loop with experience buffer | Updates network from observed data |\n",
        "| **6. Evaluation** | Compared against baselines | Measured improvement over random |\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **RewardPredictionBasePolicy** is a flexible base class for bandit policies\n",
        "2. You implement `_predict_rewards()` to get reward estimates\n",
        "3. Override `_distribution()` to customize action selection\n",
        "4. Training uses MSE loss between predicted and observed rewards\n",
        "5. Epsilon-greedy exploration helps discover better actions\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try **Thompson Sampling** for better exploration\n",
        "- Add **Upper Confidence Bound (UCB)** exploration\n",
        "- Use **TF-Agents' built-in bandit agents** (LinUCB, Neural Epsilon Greedy)\n",
        "- Scale to **real ad data** with proper feature engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéâ Notebook Complete!\")\n",
        "print(\"\\nYou've learned how to:\")\n",
        "print(\"  ‚úÖ Create a contextual bandit environment for ads\")\n",
        "print(\"  ‚úÖ Build a reward prediction network\")\n",
        "print(\"  ‚úÖ Implement RewardPredictionBasePolicy\")\n",
        "print(\"  ‚úÖ Train and evaluate bandit policies\")\n",
        "print(\"\\nüöÄ Now try modifying the code for your own use case!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
